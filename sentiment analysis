#Load the libraries
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import nltk
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelBinarizer
from sklearn.utils import shuffle
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from wordcloud import WordCloud,STOPWORDS
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize,sent_tokenize
from bs4 import BeautifulSoup
import spacy
import re,string,unicodedata
from nltk.tokenize.toktok import ToktokTokenizer
from nltk.stem import LancasterStemmer,WordNetLemmatizer
from sklearn.linear_model import LogisticRegression,SGDClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from textblob import TextBlob
from textblob import Word
from sklearn.metrics import classification_report,confusion_matrix,accuracy_score

import os
print(os.listdir("/Users/kk/Desktop/py/"))
import warnings
warnings.filterwarnings('ignore')



# Filter out positive and negative reviews
rotten_data = pd.read_csv('/Users/kk/Desktop/py/films_new.csv')
positive_reviews = rotten_data[rotten_data['scoreSentiment'] == 'POSITIVE']
negative_reviews = rotten_data[rotten_data['scoreSentiment'] == 'NEGATIVE']

# Randomly pick the same number of positive and negative reviews
num_samples = min(len(positive_reviews), len(negative_reviews))
random_positive_reviews = positive_reviews.sample(n=num_samples, random_state=42)
random_negative_reviews = negative_reviews.sample(n=num_samples, random_state=42)

# Merge positive and negative comments into a new DataFrame
balanced_data = pd.concat([random_positive_reviews, random_negative_reviews])

# At this point, the same number of positive and negative comments are included in the balanced_data
print(random_positive_reviews)
print(random_negative_reviews)

#shuffle the dataset randomly
balanced_data = shuffle(balanced_data, random_state=42)

balanced_data.to_csv('/Users/kk/Desktop/py/random_data.csv', index=False)



#importing the training data
print(balanced_data.shape)
print(balanced_data.head(5))

#Summary of the dataset
description = balanced_data.describe()
print(description)

#sentiment count
print(balanced_data['scoreSentiment'].value_counts())

#split the dataset
#train dataset
train_reviews = balanced_data.reviewText[:623688]
train_sentiments = balanced_data.scoreSentiment[:623688]
#test dataset
test_reviews = balanced_data.reviewText[623688:]
test_sentiments = balanced_data.scoreSentiment[623688:]
print(train_reviews.shape,train_sentiments.shape)
print(test_reviews.shape,test_sentiments.shape)





import ssl

try:
    _create_unverified_https_context = ssl._create_unverified_context
except AttributeError:
    pass
else:
    ssl._create_default_https_context = _create_unverified_https_context

# Download NLTK resources if you haven't already
nltk.download('punkt')
nltk.download('stopwords')

# Initialize the tokenizer
tokenizer = ToktokTokenizer()

# Setting English stopwords
stopword_list = nltk.corpus.stopwords.words('english')

#Removing the html strips
def strip_html(text):
    soup = BeautifulSoup(text, "html.parser")
    return soup.get_text()

#Removing the square brackets
def remove_between_square_brackets(text):
    return re.sub('\[[^]]*\]', '', text)

#Removing the noisy text
def denoise_text(text):
    text = strip_html(text)
    text = remove_between_square_brackets(text)
    return text
#Apply function on review column
balanced_data['reviewText']= balanced_data['reviewText'].apply(denoise_text)

#Define function for removing special characters
def remove_special_characters(text, remove_digits = True):
    pattern=r'[^a-zA-z0-9\s]'
    text=re.sub(pattern,'',text)
    return text
#Apply function on review column
balanced_data['reviewText']=balanced_data['reviewText'].apply(remove_special_characters)


#set stopwords to english
stop=set(stopwords.words('english'))
print(stop)

#removing the stopwords
def remove_stopwords(text, is_lower_case=False):
    tokens = tokenizer.tokenize(text)
    tokens = [token.strip() for token in tokens]
    if is_lower_case:
        filtered_tokens = [token for token in tokens if token not in stopword_list]
    else:
        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
    filtered_text = ' '.join(filtered_tokens)
    return filtered_text
#Apply function on review column
balanced_data['reviewText']=balanced_data['reviewText'].apply(remove_stopwords)




#normalized train reviews
norm_train_reviews = balanced_data.reviewText[:623688]

#convert dataframe to string
#norm_train_string=norm_train_reviews.to_string()
#Spelling correction using Textblob
#norm_train_spelling=TextBlob(norm_train_string)
#norm_train_spelling.correct()
#Tokenization using Textblob
#norm_train_words=norm_train_spelling.words
#norm_train_words

#Normalized test reviews
norm_test_reviews=balanced_data.reviewText[623688:]

##convert dataframe to string
#norm_test_string=norm_test_reviews.to_string()
#spelling correction using Textblob
#norm_test_spelling=TextBlob(norm_test_string)
#print(norm_test_spelling.correct())
#Tokenization using Textblob
#norm_test_words=norm_test_spelling.words
#norm_test_words

#Tfidf vectorizer
tv=TfidfVectorizer(min_df=0.0000001,max_df=1.0000000,use_idf=True,ngram_range=(1,3))
#transformed train reviews
# 对特征进行切片以匹配训练和测试数据集的数量
tv_train_reviews = tv.fit_transform(norm_train_reviews)
tv_test_reviews = tv.transform(norm_test_reviews)

print('Tfidf_train:',tv_train_reviews.shape)
print('Tfidf_test:',tv_test_reviews.shape)

#labeling the sentiment data
lb=LabelBinarizer()
#transformed sentiment data
sentiment_data=lb.fit_transform(balanced_data['scoreSentiment'])
print(sentiment_data.shape)

#Spliting the sentiment data
train_sentiments=sentiment_data[:623688]
test_sentiments=sentiment_data[623688:]
print(train_sentiments)
print(test_sentiments)


#training the model
lr=LogisticRegression(penalty='l2',max_iter=500,C=1,random_state=42)
#Fitting the model for tfidf features
lr_tfidf=lr.fit(tv_train_reviews,train_sentiments)
print(lr_tfidf)

##Predicting the model for tfidf features
lr_tfidf_predict=lr.predict(tv_test_reviews)
print(lr_tfidf_predict)

#Accuracy score for tfidf features
lr_tfidf_score=accuracy_score(test_sentiments,lr_tfidf_predict)
print("lr_tfidf_score :",lr_tfidf_score)

#Classification report for tfidf features
lr_tfidf_report=classification_report(test_sentiments,
lr_tfidf_predict,target_names=['Positive','Negative'])
print(lr_tfidf_report)

#confusion matrix for tfidf features
cm_tfidf=confusion_matrix(test_sentiments,lr_tfidf_predict,labels=[1,0])
print(cm_tfidf)

# 创建一个空的情感标签列表
predicted_sentiments = []

# 将预测值转换为情感标签
for prediction in lr_tfidf_predict:
    if prediction == 1:
        predicted_sentiments.append("Positive")
    else:
        predicted_sentiments.append("Negative")

test_data = pd.DataFrame({
    'id': test_reviews.index,  # 使用索引作为 id 列
    'creationDate': balanced_data.loc[test_reviews.index, 'creationDate'],
    'publicatioName': balanced_data.loc[test_reviews.index, 'publicatioName'],
    'reviewText': balanced_data.loc[test_reviews.index, 'reviewText'],
    'scoreSentiment': balanced_data.loc[test_reviews.index, 'scoreSentiment'],
    'title': balanced_data.loc[test_reviews.index, 'title'],
    'runtimeMinutes': balanced_data.loc[test_reviews.index, 'runtimeMinutes'],
    'genre': balanced_data.loc[test_reviews.index, 'genre'],
    'originalLanguage': balanced_data.loc[test_reviews.index, 'originalLanguage'],
    'predictedSentiment': predicted_sentiments
})


# 将DataFrame导出到CSV文件
test_data.to_csv('/Users/kk/Desktop/py/test_data_with_predictions.csv', index=False)
